{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myOwnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# path to your own data and coco file\n",
    "train_data_dir = './frames/'\n",
    "train_coco = './merged_coco_annotation/merged_coco.json'\n",
    "\n",
    "# create own Dataset\n",
    "my_dataset = myOwnDataset(root=train_data_dir,\n",
    "                          annotation=train_coco,\n",
    "                          )\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Batch size\n",
    "train_batch_size = 4\n",
    "\n",
    "# own DataLoader\n",
    "# solve the issue of \n",
    "data_loader = torch.utils.data.DataLoader(my_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3840x2160 at 0x7F86916F72E0>,\n",
       " {'boxes': tensor([[1626.5850,  148.1710, 1695.7321,  250.2440],\n",
       "          [2011.8290,  701.3410, 2074.3899,  823.1710],\n",
       "          [1687.6130,  137.0280, 1691.2190,  147.8460]]),\n",
       "  'labels': tensor([1, 1, 1]),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([7058.0415, 7621.8066,   39.0097]),\n",
       "  'iscrowd': tensor([0, 0, 0])})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3840x2160 at 0x7F86916DC790>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3840x2160 at 0x7F8660BE4430>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3840x2160 at 0x7F8660BE47C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3840x2160 at 0x7F8660BE4130>)\n",
      "({'boxes': tensor([[2122.2710,  801.7470, 2178.8650,  914.9340],\n",
      "        [1235.6331, 1329.9561, 1301.6591, 1433.7120]]), 'labels': tensor([1, 1]), 'image_id': tensor([7775]), 'area': tensor([6405.7051, 6850.5938]), 'iscrowd': tensor([0, 0])}, {'boxes': tensor([[2188.8000,  182.4000, 2371.2000,  441.6000],\n",
      "        [2323.2000,  604.8000, 2496.0000,  873.6000],\n",
      "        [3427.2000, 1555.2000, 3600.0000, 1680.0000]]), 'labels': tensor([1, 1, 1]), 'image_id': tensor([7936]), 'area': tensor([47278.0781, 46448.6406, 21565.4395]), 'iscrowd': tensor([0, 0, 0])}, {'boxes': tensor([[ 518.1080, 1090.9460,  795.4050, 1298.9189],\n",
      "        [1685.6760, 1072.7030, 1981.2159, 1382.8380],\n",
      "        [ 116.7570, 1568.9189,  386.7570, 1809.7300]]), 'labels': tensor([1, 1, 1]), 'image_id': tensor([3980]), 'area': tensor([57670.2891, 91657.2969, 65018.9688]), 'iscrowd': tensor([0, 0, 0])}, {'boxes': tensor([[1778.8235, 1051.1230, 1888.5560, 1189.7330],\n",
      "        [2419.8931,   86.6310, 2593.1550,  242.5670]]), 'labels': tensor([1, 1]), 'image_id': tensor([6984]), 'area': tensor([15210.0186, 27017.7832]), 'iscrowd': tensor([0, 0])})\n"
     ]
    }
   ],
   "source": [
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# # DataLoader is iterable over Dataset\n",
    "# for imgs, annotations in data_loader:\n",
    "#     imgs = list(img.to(device) for img in imgs)\n",
    "#     annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "#     print(annotations)\n",
    "\n",
    "train_features, train_labels = next(iter(data_loader))\n",
    "print(train_features)\n",
    "print(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b943e5280a275238bc72fa812b48b24c5e80cacf3fb51c13d96f985ad1a46fdd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs153')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
